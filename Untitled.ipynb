{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tsundere dragon            1.000000\n",
       "gon na                     0.681236\n",
       "24 episode                 0.557279\n",
       "holy shit                  0.500657\n",
       "pretty good                0.428046\n",
       "hard drive                 0.424891\n",
       "good episode               0.372300\n",
       "8 bit                      0.336441\n",
       "damn fun                   0.318283\n",
       "great sage                 0.312242\n",
       "episode good               0.304919\n",
       "na watch                   0.298518\n",
       "anime season               0.289358\n",
       "manga good                 0.267627\n",
       "ln manga                   0.266896\n",
       "high quality               0.265223\n",
       "animation good             0.264269\n",
       "good anime                 0.263402\n",
       "pleasantly surprise        0.263016\n",
       "find anime                 0.261398\n",
       "manga reader               0.259875\n",
       "op overlord                0.259292\n",
       "episode hype               0.256926\n",
       "anime adaptation           0.249019\n",
       "reincarnate slime          0.247461\n",
       "production quality         0.234982\n",
       "watch episode              0.231384\n",
       "isekai anime               0.231332\n",
       "1st episode                0.225183\n",
       "spider good                0.221057\n",
       "                             ...   \n",
       "existence slime            0.015902\n",
       "thought interested         0.015902\n",
       "normal human               0.015902\n",
       "structure reincarnation    0.015902\n",
       "human living               0.015902\n",
       "example character          0.015902\n",
       "thrust mysterious          0.015902\n",
       "development conflict       0.015902\n",
       "light isekai               0.015902\n",
       "simple element             0.015902\n",
       "see misguide               0.015902\n",
       "challenge main             0.015902\n",
       "desire explain             0.015902\n",
       "singular simple            0.015902\n",
       "deliver gradually          0.015902\n",
       "gradually show             0.015902\n",
       "topic character            0.015902\n",
       "harem profit               0.015902\n",
       "point compare              0.015902\n",
       "character thrust           0.015902\n",
       "trap world                 0.015902\n",
       "way present                0.015902\n",
       "eats breaths               0.015902\n",
       "fantasy genre              0.015902\n",
       "explain eats               0.015902\n",
       "fantasy element            0.015902\n",
       "reincarnation explain      0.015902\n",
       "day character              0.015902\n",
       "explain rpg                0.015902\n",
       "aspect literal             0.015902\n",
       "Length: 4913, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "Kaguya_sama_max_ep = 13\n",
    "Boogiepop_max_ep = 19\n",
    "Slime_anime_max_ep = 2\n",
    "\n",
    "def penntag(pen):\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[pen[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "for ep in range(1,Slime_anime_max_ep):\n",
    "\n",
    " #kaguya_file = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Manually determines\\Kaguya-sama cleaned\\Kaguya-sama Episode ' +str(3)+' .csv'\n",
    " \n",
    " #print('File '+str(ep)+ ' read')\n",
    "# df = pd.read_csv(kaguya_file,index_col=0,encoding='cp1252')\n",
    "\n",
    " kaguya_file = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Manually determines\\Tensei Slime cleaned\\Tensei Slime Episode ' + str(\n",
    "        1) + ' .csv'\n",
    "\n",
    " df = pd.read_csv(kaguya_file, index_col=0, encoding='utf-8-sig')\n",
    " pos_df = df[df['Actual Polarity'] == 1]\n",
    " pos_df = pos_df.reset_index(drop=True)\n",
    "\n",
    "# print(pos_df)\n",
    " # Convert dataframe values into string\n",
    " pos_df['Comments'] = pos_df['Comments'].astype(str)\n",
    "\n",
    " #Remove punctuation marks and tokenize each and every word\n",
    " pos_df['Comments'] = pos_df['Comments'].str.replace('[^\\w\\s]',' ')\n",
    " comment_words = pos_df['Comments'].apply(word_tokenize)\n",
    "    \n",
    " lemma = WordNetLemmatizer()\n",
    " stopword = set(stopwords.words('english'))\n",
    "\n",
    " # Put words into lowercase and remove stopwords\n",
    " lowercase_words = comment_words.apply(lambda x : [word.lower() for word in x])\n",
    " words_without_stopwords = lowercase_words.apply(lambda x : [word for word in x if word not in stopword])\n",
    "\n",
    " # remove further stopwords\n",
    " stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    " stopwords_jsa = set(stopwords_json['en'])\n",
    " stopword_removed = words_without_stopwords.apply(lambda x : [word for word in x if word not in stopwords_jsa])\n",
    "\n",
    " #Apply part-of-speech to tokenized words\n",
    " POS_words = stopword_removed.apply(nltk.pos_tag)\n",
    " samples = POS_words[:POS_words.shape[0]]\n",
    "\n",
    " list_of_lemmad_words = []\n",
    "\n",
    " for i in range(0,samples.shape[0]):\n",
    "  #lemmatize the tokens using Penn Treebank POS tokens in the penntag() function\n",
    "  lemma_words = [lemma.lemmatize(words,pos=penntag(tags)) for words,tags in samples[i]]\n",
    "  list_of_lemmad_words.append([lemma_words])\n",
    "\n",
    "  tokeniz = pd.DataFrame(list_of_lemmad_words,columns=['Values'])\n",
    "\n",
    " def return_back_df(doc):\n",
    "    return doc\n",
    "\n",
    " #perfom TF-IDF on the tokens\n",
    " token_lemma_values = tokeniz['Values']\n",
    " tfidf = TfidfVectorizer(analyzer='word',preprocessor=return_back_df,tokenizer=return_back_df,ngram_range=(2,2))\n",
    " response = tfidf.fit_transform(token_lemma_values)\n",
    "\n",
    " feature_names = tfidf.get_feature_names()\n",
    "\n",
    " dense = response.todense()\n",
    " wordlist = dense.tolist()\n",
    "\n",
    " #Put TF-IDF values onto the dataframe and sum up the values\n",
    " tfif_val = pd.DataFrame(wordlist,columns=feature_names)\n",
    " sum_of_tfidf = tfif_val.sum(numeric_only =True)\n",
    "\n",
    " \n",
    " #Normalize and sort the TF-IDF values\n",
    " #normalized_values = (sum_of_tfidf - sum_of_tfidf.mean())/(sum_of_tfidf.max() - sum_of_tfidf.min())\n",
    " #sorted_norms = normalized_values.sort_values(ascending=False)\n",
    "\n",
    "\n",
    " #csv_file_name_kaguya = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Tokenizer\\Kaguya-sama TF-IDF\\TF-IDF words of Kaguya-sama Episode ' +str(ep)+' .csv'\n",
    " #csv_file_name_boogiepop = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Tokenizer\\Boogiepop Wa Waranai TF-IDF\\TF-IDF words of Boogiepop Wa Waranai Episode ' + str(ep) + ' .csv'\n",
    " #csv_file_name_slime = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Tokenizer\\Tensei Slime TF-IDF\\TF-IDF words of Tensei Slime Episode ' +str(ep)+' .csv'\n",
    "\n",
    " #Write the TF-IDF values to the file.\n",
    " #print('File ' + str(ep) + ' Written\\n')\n",
    " #sorted_norms.to_csv(csv_file_name_slime,encoding='utf-8-sig')\n",
    " sum_of_tfidf.sort_values(ascending=False)\n",
    " normalized = sum_of_tfidf/sum_of_tfidf.max()\n",
    " ur = normalized.sort_values(ascending=False)\n",
    " display(ur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "Kaguya_sama_max_ep = 13\n",
    "Boogiepop_max_ep = 19\n",
    "Slime_anime_max_ep = 2\n",
    "\n",
    "def penntag(pen):\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[pen[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "for ep in range(1,Slime_anime_max_ep):\n",
    "\n",
    " kaguya_file = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Manually determines\\Tensei Slime cleaned\\Tensei Slime Episode ' + str(\n",
    "        1) + ' .csv'\n",
    "\n",
    " df = pd.read_csv(kaguya_file, index_col=0, encoding='utf-8-sig')\n",
    " pos_df = df[df['Actual Polarity'] == 0]\n",
    " pos_df = pos_df.reset_index(drop=True)\n",
    "\n",
    "# print(pos_df)\n",
    " # Convert dataframe values into string\n",
    " pos_df['Comments'] = pos_df['Comments'].astype(str)\n",
    "\n",
    " #Remove punctuation marks and tokenize each and every word\n",
    " pos_df['Comments'] = pos_df['Comments'].str.replace('[^\\w\\s]',' ')\n",
    " comment_words = pos_df['Comments'].apply(word_tokenize)\n",
    "    \n",
    " lemma = WordNetLemmatizer()\n",
    " stopword = set(stopwords.words('english'))\n",
    "\n",
    " # Put words into lowercase and remove stopwords\n",
    " lowercase_words = comment_words.apply(lambda x : [word.lower() for word in x])\n",
    " words_without_stopwords = lowercase_words.apply(lambda x : [word for word in x if word not in stopword])\n",
    "\n",
    " # remove further stopwords\n",
    " stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    " stopwords_jsa = set(stopwords_json['en'])\n",
    " stopword_removed = words_without_stopwords.apply(lambda x : [word for word in x if word not in stopwords_jsa])\n",
    "\n",
    " #Apply part-of-speech to tokenized words\n",
    " POS_words = stopword_removed.apply(nltk.pos_tag)\n",
    " samples = POS_words[:POS_words.shape[0]]\n",
    "\n",
    " list_of_lemmad_words = []\n",
    "\n",
    " for i in range(0,samples.shape[0]):\n",
    "  #lemmatize the tokens using Penn Treebank POS tokens in the penntag() function\n",
    "  lemma_words = [lemma.lemmatize(words,pos=penntag(tags)) for words,tags in samples[i]]\n",
    "  list_of_lemmad_words.append([lemma_words])\n",
    "\n",
    "  tokeniz = pd.DataFrame(list_of_lemmad_words,columns=['Values'])\n",
    " \n",
    " #print(list_of_lemmad_words)\n",
    "\n",
    " def return_back_df(doc):\n",
    "    return doc\n",
    "\n",
    " #perfom TF-IDF on the tokens\n",
    " token_lemma_values = tokeniz['Values']\n",
    "# print(tokeniz['Values'])\n",
    " tfidf = TfidfVectorizer(analyzer='word',preprocessor=return_back_df,tokenizer=return_back_df,ngram_range=(2,2))\n",
    " response = tfidf.fit_transform(token_lemma_values)\n",
    "\n",
    " feature_names = tfidf.get_feature_names()\n",
    "\n",
    " dense = response.todense()\n",
    " wordlist = dense.tolist()\n",
    "\n",
    " #print(wordlist)\n",
    "\n",
    " #Put TF-IDF values onto the dataframe and sum up the values\n",
    " tfif_val = pd.DataFrame(wordlist,columns=feature_names)\n",
    " sum_of_tfidf = tfif_val.sum(numeric_only =True)\n",
    "\n",
    " \n",
    " #Normalize and sort the TF-IDF values\n",
    " #normalized_values = (sum_of_tfidf - sum_of_tfidf.mean())/(sum_of_tfidf.max() - sum_of_tfidf.min())\n",
    " #sorted_norms = normalized_values.sort_values(ascending=False)\n",
    "\n",
    "\n",
    " #csv_file_name_kaguya = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Tokenizer\\Kaguya-sama TF-IDF\\TF-IDF words of Kaguya-sama Episode ' +str(ep)+' .csv'\n",
    " #csv_file_name_boogiepop = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Tokenizer\\Boogiepop Wa Waranai TF-IDF\\TF-IDF words of Boogiepop Wa Waranai Episode ' + str(ep) + ' .csv'\n",
    " #csv_file_name_slime = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Tokenizer\\Tensei Slime TF-IDF\\TF-IDF words of Tensei Slime Episode ' +str(ep)+' .csv'\n",
    "\n",
    " #Write the TF-IDF values to the file.\n",
    " #print('File ' + str(ep) + ' Written\\n')\n",
    " #sorted_norms.to_csv(csv_file_name_slime,encoding='utf-8-sig')\n",
    " sum_of_tfidf.sort_values(ascending=False)\n",
    " normalized = sum_of_tfidf/sum_of_tfidf.max()\n",
    " normalized.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8417721518987342\n",
      "0.8433476394849786\n",
      "0.9949367088607595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Actual Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Well thats an overreaction You can hate the ones from GS but why hate these guys And we have the Enri and her likeable Goblins in Overlord too</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Going from Goblin hate to Direwolf hate lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Okay but I didnt like Smart Phone either and thought it was a trash tier show So comparing it against that is pretty much on par with comparing it with master of ragnarok Yeah sure its better than that but that doesnt SAY anything    Also I HAVE read the manga even if I havent read the light novel He never runs into a real challenge at any point some things might look like challenges but they arent I watched the anime hoping for an improvement but nope its all the worst aspects of Isekai and utterly loathsome In addition within the next 12 episodes he gets a human form completely negating any purpose to him being a slime to begin with     Sadly the author never goes anywhere with this idea We have an overpowered godlike isekai wish fullfillment with no goals or real conflicts who everyone flocks to because hes strong and cool I dont really get the following this has considering how blase it is</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>I hate how comitties get away with this</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>I had hope for madhouse considering hunterxhunter But nope even they fucked up overlord</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>I actually read the first chapter of the manga They are not just animalistic They are total sadists Remember when that one goblin broke the mage staff of the magician girl in half The expression on that goblins face says it all He was totally getting off to destroying everything that his victim holds dear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>dont remember me the bad CGI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>aw man dont make me feel bad about those goblins after watching goblin slayer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Comments  \\\n",
       "346  Well thats an overreaction You can hate the ones from GS but why hate these guys And we have the Enri and her likeable Goblins in Overlord too                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "354  Going from Goblin hate to Direwolf hate lol                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "419  Okay but I didnt like Smart Phone either and thought it was a trash tier show So comparing it against that is pretty much on par with comparing it with master of ragnarok Yeah sure its better than that but that doesnt SAY anything    Also I HAVE read the manga even if I havent read the light novel He never runs into a real challenge at any point some things might look like challenges but they arent I watched the anime hoping for an improvement but nope its all the worst aspects of Isekai and utterly loathsome In addition within the next 12 episodes he gets a human form completely negating any purpose to him being a slime to begin with     Sadly the author never goes anywhere with this idea We have an overpowered godlike isekai wish fullfillment with no goals or real conflicts who everyone flocks to because hes strong and cool I dont really get the following this has considering how blase it is   \n",
       "426  I hate how comitties get away with this                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "428  I had hope for madhouse considering hunterxhunter But nope even they fucked up overlord                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "435  I actually read the first chapter of the manga They are not just animalistic They are total sadists Remember when that one goblin broke the mage staff of the magician girl in half The expression on that goblins face says it all He was totally getting off to destroying everything that his victim holds dear                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "449  dont remember me the bad CGI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "464  aw man dont make me feel bad about those goblins after watching goblin slayer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "\n",
       "     Actual Polarity  \n",
       "346  0                \n",
       "354  0                \n",
       "419  0                \n",
       "426  0                \n",
       "428  0                \n",
       "435  0                \n",
       "449  0                \n",
       "464  0                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample,shuffle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score,f1_score, accuracy_score,classification_report\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as pypl\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# boogiepop_file = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Data Cleaner\\Boogiepop cleaned\\Cleaned_Boogiepop_Wa_Waranai_Episode_' + str(ep) + '_Comment_list.csv'\n",
    "# kaguya_file = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Data Cleaner\\Kaguya-sama cleaned\\Cleaned_Kaguya_sama_Episode_' + str(1) + '_Comment_list.csv'\n",
    "# slime_file = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Data Cleaner\\Tensei Slime cleaned\\Cleaned_Tensei_Slime_Episode_' + str(ep) + '_Comment_list.csv'\n",
    "\n",
    "\n",
    "def penntag(pen):\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[pen[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def comment_cleaner(comm, comment_array):\n",
    "    temp_comm = []\n",
    "    stopwords_removed = [word for word in comm.lower().split() if word not in stopword]\n",
    "    POS_words = nltk.pos_tag(stopwords_removed)\n",
    "    for i in range(0, len(POS_words)):\n",
    "        lemmas = lemma.lemmatize(POS_words[i][0], pos=penntag(POS_words[i][1]))\n",
    "        temp_comm.append(lemmas)\n",
    "    #print(temp_comm)\n",
    "    megos = ' '.join(word for word in temp_comm)\n",
    "   # print(megos)\n",
    "    comment_array.append(temp_comm)\n",
    "    return comment_array\n",
    "    temp_comm.clear()\n",
    "\n",
    "for ep in range(1, 2):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    # kaguya_file = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Manually determines\\Kaguya-sama cleaned\\Kaguya-sama Episode ' +str(1)+' .csv'\n",
    "    kaguya_file = 'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Manually determines\\Tensei Slime cleaned\\Tensei Slime Episode ' + str(\n",
    "        1) + ' .csv'\n",
    "\n",
    "    df = pd.read_csv(kaguya_file, index_col=0, encoding='utf-8-sig')\n",
    "    #df = df.sample(frac=1)\n",
    "    # Convert dataframe values into string\n",
    "    df['Comments'] = df['Comments'].astype(str)\n",
    "\n",
    "    # Remove punctuation marks and tokenize each and every word\n",
    "    df['Comments'] = df['Comments'].str.replace('[^\\w\\s]', '')\n",
    "\n",
    "    # Split into positive and negative datasets\n",
    "    pos_df = df[df['Actual Polarity'] == 1]\n",
    "    neg_df = df[df['Actual Polarity'] == 0]\n",
    "    neu_df = df[df['Actual Polarity'] == 2]\n",
    "\n",
    "    neg_upsample = resample(neg_df, replace=True, n_samples=len(pos_df),random_state=0)\n",
    "    neu_upsample = resample(neu_df,replace=True,n_samples=len(pos_df))\n",
    "\n",
    "    # Concatenate them into one\n",
    "    train_df = pd.concat([pos_df, neg_upsample])\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    comment_array = []\n",
    "    train_target = []\n",
    "    comtest_array = []\n",
    "\n",
    "    for i in range(0, int(train_df.shape[0])):\n",
    "        sentences = train_df['Comments'][i]\n",
    "        train_words = comment_cleaner(sentences, comment_array)\n",
    "\n",
    "    def return_back_df(doc):\n",
    "        return doc\n",
    "\n",
    "\n",
    "    vec = TfidfVectorizer(analyzer='word', preprocessor=return_back_df, tokenizer=return_back_df,ngram_range=(1,2),use_idf=True,norm='l2')\n",
    "    x = vec.fit_transform(train_words)\n",
    "    train_target = train_df['Actual Polarity'][0:int(train_df.shape[0])]\n",
    "    '''\n",
    "    res = x.todense()\n",
    "    ges = res.tolist()\n",
    "    vo = vec.get_feature_names()\n",
    "\n",
    "    tval = pd.DataFrame(ges,columns=vo)\n",
    "    sum = tval.sum(numeric_only =True)\n",
    "\n",
    "    sum.sort_values(ascending=False)\n",
    "    normalized = sum / sum.max()\n",
    "    print(normalized.sort_values(ascending=False))\n",
    "    '''\n",
    "    #x,train_target = shuffle(x,train_target)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, train_target, test_size=0.3,random_state=1)\n",
    "    #Naive Bayes test\n",
    "    scores = {'Accuracy': make_scorer(accuracy_score),\n",
    "              'Precision': make_scorer(precision_score),\n",
    "              'Recall': make_scorer(recall_score),\n",
    "              'F1-Score': make_scorer(f1_score)\n",
    "              }\n",
    "    clas_linear = svm.SVC(kernel='linear', C=75.0, gamma=0.001)\n",
    "    clas_linear.fit(x,train_target)\n",
    "   \n",
    "    #NB = MultinomialNB()\n",
    "    #NB = BernoulliNB()\n",
    "\n",
    "    #NB = GaussianNB()\n",
    "    #NB.fit(x,train_target)\n",
    "\n",
    "\n",
    "    '''\n",
    "    xu = cross_validate(NB, x, train_target, cv=10, scoring=scores,return_train_score=True)\n",
    "    print('Training accuracy :', np.mean(xu['train_Accuracy']))\n",
    "    print('Training precision :', np.mean(xu['train_Precision']))\n",
    "    print('Training recall :', np.mean(xu['train_Recall']))\n",
    "    print('Training F1-Score :', np.mean(xu['train_F1-Score']))\n",
    "    print('\\n')\n",
    "    print('Testing accuracy :', np.mean(xu['test_Accuracy']))\n",
    "    print('Testing precision :', np.mean(xu['test_Precision']))\n",
    "    print('Testing recall :', np.mean(xu['test_Recall']))\n",
    "    print('Testing F1-Score :', np.mean(xu['test_F1-Score']))\n",
    "    #NB.fit(x,train_target)\n",
    "    y_pred = NB.predict(x_test)\n",
    "\n",
    "    #print(accuracy_score(y_test,y_pred))\n",
    "   # print(precision_score(y_test,y_pred))\n",
    "    #print(recall_score(y_test,y_pred))\n",
    "    '''\n",
    "\n",
    "    test_file = pd.read_csv(\n",
    "        'D:\\Github Projects\\Heriot-Watt-Msc-Project-Sentiment-Analysis\\Manually determines\\Tensei Slime cleaned\\Tensei Slime Episode ' + str(\n",
    "            2) + ' .csv', index_col=0, encoding='utf-8-sig')\n",
    "    # test_file = test_file.sample(frac=1).reset_index(drop=True)\n",
    "    # test_file = test_file[0:00]\n",
    "    test_file['Comments'] = test_file['Comments'].astype(str)\n",
    "\n",
    "    # Remove punctuation marks and tokenize each and every word\n",
    "    test_file['Comments'] = test_file['Comments'].str.replace('[^\\w\\s]', '')\n",
    "\n",
    "    pos_test_file = test_file[test_file['Actual Polarity'] == 1]\n",
    "    neg_test_file = test_file[test_file['Actual Polarity'] == 0]\n",
    "    neu_test_file = test_file[test_file['Actual Polarity'] == 2]\n",
    "\n",
    "    train_test = pd.concat([pos_test_file, neg_test_file])\n",
    "    train_test = train_test.reset_index(drop=True)\n",
    "    # train_test = train_test.sample(frac = 1)\n",
    "\n",
    "    comtest_array = []\n",
    "\n",
    "    for i in range(0, int(train_test.shape[0])):\n",
    "        sen = train_test['Comments'][i]\n",
    "        comtest_array = comment_cleaner(sen, comtest_array)\n",
    "\n",
    "    # ela_2 = comtest_array[0:300]\n",
    "    veca = TfidfVectorizer(analyzer='word', preprocessor=return_back_df, tokenizer=return_back_df, ngram_range=(1, 2),\n",
    "                          use_idf=True, norm='l2')\n",
    "    xe = vec.transform(comtest_array)\n",
    "    # print(xe)\n",
    "    ye = train_test['Actual Polarity'][0:train_test.shape[0]]\n",
    "    # print(ye)\n",
    "    '''\n",
    "    res = x.todense()\n",
    "    ges = res.tolist()\n",
    "    vo = vec.get_feature_names()\n",
    "\n",
    "    tval = pd.DataFrame(ges, columns=vo)\n",
    "    sum = tval.sum(numeric_only=True)\n",
    "\n",
    "    sum.sort_values(ascending=False)\n",
    "    normalized = sum / sum.max()\n",
    "    print(normalized.sort_values(ascending=False))\n",
    "    '''\n",
    "    # xe_train, xe_test, ye_train, ye_test = train_test_split(xe, ye, test_size=0.2, random_state=0, stratify=ye)\n",
    "    # print(xe_test)\n",
    "    # print(ye_test)\n",
    "    \n",
    "    x_pred = clas_linear.predict(xe)\n",
    "    #x_pred = NB.predict(xe)\n",
    "    #xes = cross_validate(NB,xe,ye,cv=10,return_train_score=True)\n",
    "    #print(xes)\n",
    "    print(accuracy_score(ye, x_pred))\n",
    "    print(precision_score(ye, x_pred))\n",
    "    print(recall_score(ye, x_pred))\n",
    "\n",
    "    da = {'Comments': train_test['Comments'],'Actual Polarity':x_pred}\n",
    "    xool = pd.DataFrame(da)\n",
    "    aas = xool[xool['Actual Polarity']==0]\n",
    "    ass = aas['Comments']\n",
    "    display(aas)\n",
    "    pd.set_option('display.max_rows', 131)\n",
    "    #print(classification_report(ye,x_pred,target_names=['Negative','Positive','Neutral']))\n",
    "    '''\n",
    "    er = []\n",
    "    ur = comment_cleaner('uncanny isekai',er)\n",
    "    ar = vec.transform(ur)\n",
    "    s = NB.predict(ar)\n",
    "    print(s)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
