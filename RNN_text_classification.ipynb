{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN text classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WeebMogul/Heriot-Watt-Msc-Project-Sentiment-Analysis/blob/master/RNN_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-z7MNlswuq3",
        "colab_type": "text"
      },
      "source": [
        "**Setting up the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYc7KsaHOG_4",
        "colab_type": "code",
        "outputId": "490e3e84-6f06-4d52-bf94-4456414d266d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!pip install imbalanced-learn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (0.13.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYnKFbu51YTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "86ceb872-fc7c-4adb-dba0-1a3fa4821705"
      },
      "source": [
        "#Sklearn\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV,train_test_split\n",
        "from sklearn.utils import resample,shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer,CountVectorizer\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score,f1_score, accuracy_score,classification_report,confusion_matrix\n",
        "from sklearn.metrics import make_scorer,precision_score,recall_score,accuracy_score,f1_score\n",
        "from sklearn.utils import class_weight\n",
        "#NLTK\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#Tensorflow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Activation, Embedding,Input,Bidirectional,SpatialDropout1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Other\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as pypl\n",
        "import seaborn as sd\n",
        "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TppPnkfn2VAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "b71e656b-c44f-497e-84fc-636ad82475e0"
      },
      "source": [
        " nltk.download(\"popular\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIYgi3mdrZaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contract = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMf8Wj5u1EOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "def penntag(pen):\n",
        "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
        "                  'VB': 'v', 'RB': 'r'}\n",
        "    try:\n",
        "        return morphy_tag[pen[:2]]\n",
        "    except:\n",
        "        return 'n'\n",
        "def stopword_remover(comm, comment_array):\n",
        "    megos = ' '\n",
        "    uncontracted = ' '.join([contract[word] if word in contract else word for word in comm.lower().split()])\n",
        "    stopwords_removed = [word for word in uncontracted.lower().split() if word not in stopword]\n",
        "    megos = ' '.join(word for word in stopwords_removed)\n",
        "    comment_array.append(megos)\n",
        "  #  print(comment_array)\n",
        "    return megos\n",
        "  \n",
        "def comment_cleaner(comm, comment_array):\n",
        "    temp_comm = []\n",
        "    uncontracted = ' '.join([contract[word] if word in contract else word for word in comm.lower().split()])\n",
        "    stopwords_removed = [word for word in uncontracted.lower().split() if word not in stopword]\n",
        "    POS_words = nltk.pos_tag(stopwords_removed)\n",
        "    for i in range(0, len(POS_words)):\n",
        "        lemmas = lemma.lemmatize(POS_words[i][0], pos=penntag(POS_words[i][1]))\n",
        "        temp_comm.append(lemmas)\n",
        "    # print(temp_comm)\n",
        "    megos = ' '.join(word for word in temp_comm)\n",
        "    comment_array.append(megos)\n",
        "    return megos\n",
        "    #comment_array.clear()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBSsRmqn0nk6",
        "colab_type": "text"
      },
      "source": [
        "**Loading files to the colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCErx58Tx9ch",
        "colab_type": "code",
        "outputId": "140d903e-7767-4d74-8ce3-bad9fe11e011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1o-5N6ByrgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "df1 = pd.read_csv('/content/drive/My Drive/Tensei Slime Training set/Tensei Slime Episode 8 .csv',index_col=0, encoding='utf-8-sig')\n",
        "df2 = pd.read_csv('/content/drive/My Drive/Tensei Slime Training set/Tensei Slime Episode 7 .csv',index_col=0, encoding='utf-8-sig')\n",
        "'''\n",
        "df1 = pd.read_csv('/content/drive/My Drive/Kaguya-sama cleaned/Kaguya-sama Episode 1 .csv',index_col=0, encoding='utf-8-sig')\n",
        "df2 = pd.read_csv('/content/drive/My Drive/Kaguya-sama cleaned/Kaguya-sama Episode 2 .csv',index_col=0, encoding='utf-8-sig')\n",
        "df3 = pd.read_csv('/content/drive/My Drive/Kaguya-sama cleaned/Kaguya-sama Episode 3 .csv',index_col=0, encoding='utf-8-sig')\n",
        "df4 = pd.read_csv('/content/drive/My Drive/Kaguya-sama cleaned/Kaguya-sama Episode 4 .csv',index_col=0, encoding='utf-8-sig')\n",
        "df5 = pd.read_csv('/content/drive/My Drive/Kaguya-sama cleaned/Kaguya-sama Episode 5 .csv',index_col=0, encoding='utf-8-sig')\n",
        "df6 = pd.read_csv('/content/drive/My Drive/Kaguya-sama cleaned/Kaguya-sama Episode 6 .csv',index_col=0, encoding='utf-8-sig')\n",
        "df7 = pd.read_csv('/content/drive/My Drive/Kaguya-sama cleaned/Kaguya-sama Episode 7 .csv',index_col=0, encoding='utf-8-sig')\n",
        "df8 = pd.read_csv('/content/drive/My Drive/Kaguya-sama cleaned/Kaguya-sama Episode 8 .csv',index_col=0, encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "df12 = pd.concat([df1,df2,df3,df4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML2n7I9VUK0P",
        "colab_type": "text"
      },
      "source": [
        "**Creating the LSTM model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm-5feXlwQlk",
        "colab_type": "code",
        "outputId": "c615cad1-51d4-4428-c6d2-bbb51ce7b1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "df12['Comment'] = df12['Comment'].astype(str)\n",
        "\n",
        "    # Remove punctuation marks and tokenize each and every word\n",
        "\n",
        "#df12['Comment'] = df12['Comment'].apply(lambda s: comment_cleaner(s, train_array))\n",
        "\n",
        "train_array = []\n",
        "test_array = []\n",
        "train_target = []\n",
        "comtest_array = []\n",
        "\n",
        "    # Remove punctuation marks and tokenize each and every word\n",
        "df12['Comment'] = df12['Comment'].str.replace('[^\\w\\s]', ' ')\n",
        "df12['Comment'] = df12['Comment'].str.replace('[\\d+]', ' ')\n",
        "df12['Comment'] = df12['Comment'].str.replace('(^| ).(( ).)*( |$)', ' ')\n",
        "\n",
        "df12['Comment'] = df12['Comment'].apply(lambda s : comment_cleaner(s, train_array))\n",
        "\n",
        "\n",
        "    # Split into positive and negative datasets\n",
        "pos_df = df12[df12['Actual Polarity'] == 1]\n",
        "neg_df = df12[df12['Actual Polarity'] == 0]\n",
        "neu_df = df12[df12['Actual Polarity'] == 2]\n",
        "    \n",
        "neu_df['Length'] = neu_df['Comment'].str.lower().str.split().apply(len)\n",
        "neu_df = neu_df[neu_df['Length'] > 15]\n",
        "\n",
        "    # Concatenate them into one\n",
        "#train_df = pd.concat([pos_df, neg_upsample])\n",
        "train_df = pd.concat([pos_df, neg_df])\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "    \n",
        "tk = Tokenizer(lower = True,num_words = 20000)\n",
        "tk.fit_on_texts(train_df['Comment'])\n",
        "tr_seq = tk.texts_to_sequences(train_df['Comment']) \n",
        "   # x_train, x_test, y_train,y_test = train_test_split(tr_seq, train_df['Actual Polarity'], test_size=0.2,random_state=22)\n",
        "tr_x = pad_sequences(tr_seq)\n",
        "    \n",
        "print(tr_x.shape)\n",
        "print(train_df['Actual Polarity'].shape)\n",
        "    \n",
        "x_train, x_test, y_train,y_test = train_test_split(tr_x, train_df['Actual Polarity'], test_size=0.2,random_state=22)\n",
        "    \n",
        "sm = RandomOverSampler(random_state=77)\n",
        "X_train, Y_train = sm.fit_sample(x_train, y_train)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2858\n",
            "371\n",
            "(3229, 268)\n",
            "(3229,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfK14pmt1r93",
        "colab_type": "code",
        "outputId": "ceb0ce5e-ef6e-4cc8-b04a-c98a7d6b39a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "import keras\n",
        "custom_adam = keras.optimizers.Adam(lr=0.00001)\n",
        "custom_sgd = keras.optimizers.SGD(lr=0.01)\n",
        "\n",
        "# batch = 200\n",
        "batch = 5\n",
        "model = Sequential()\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "print(vocabulary_size)\n",
        "embed_size = 64\n",
        "max_words = 50\n",
        "model.add(Embedding(vocabulary_size,embed_size,input_length = x_train.shape[1]))\n",
        "model.add(LSTM(units=40,return_sequences=True))\n",
        "model.add(LSTM(units=20,return_sequences=False))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=custom_sgd, metrics = ['accuracy'])\n",
        "#model.compile(loss='sparse_categorical_crossentropy', optimizer=custom_adam, metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train,Y_train,batch_size=25,epochs=10,verbose=1,validation_data = (x_test,y_test))\n",
        "#model.fit(x_train_tfidf,y_train,batch_size=20,epochs=15,verbose=1,validation_data = (xval_tfidf,y_test),callbacks=[EarlyStopping(monitor='val_loss')])\n",
        "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
        "print(accuracy)\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(accuracy)\n",
        "xe= model.predict_classes(x_test)\n",
        "print(classification_report(y_test,xe))\n",
        "print(confusion_matrix(y_test,xe))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5473\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 268, 64)           350272    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 268, 40)           16800     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 20)                4880      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 371,973\n",
            "Trainable params: 371,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 4582 samples, validate on 646 samples\n",
            "Epoch 1/10\n",
            "4582/4582 [==============================] - 72s 16ms/step - loss: 0.6935 - acc: 0.4838 - val_loss: 0.6921 - val_acc: 0.5867\n",
            "Epoch 2/10\n",
            "4582/4582 [==============================] - 69s 15ms/step - loss: 0.6933 - acc: 0.4926 - val_loss: 0.6945 - val_acc: 0.3932\n",
            "Epoch 3/10\n",
            "4582/4582 [==============================] - 69s 15ms/step - loss: 0.6932 - acc: 0.5063 - val_loss: 0.6851 - val_acc: 0.8700\n",
            "Epoch 4/10\n",
            "4582/4582 [==============================] - 69s 15ms/step - loss: 0.6931 - acc: 0.5120 - val_loss: 0.6949 - val_acc: 0.4087\n",
            "Epoch 5/10\n",
            "4582/4582 [==============================] - 71s 15ms/step - loss: 0.6930 - acc: 0.5170 - val_loss: 0.6944 - val_acc: 0.4443\n",
            "Epoch 6/10\n",
            "4582/4582 [==============================] - 70s 15ms/step - loss: 0.6928 - acc: 0.5083 - val_loss: 0.6868 - val_acc: 0.6579\n",
            "Epoch 7/10\n",
            "4582/4582 [==============================] - 70s 15ms/step - loss: 0.6928 - acc: 0.5271 - val_loss: 0.6929 - val_acc: 0.4923\n",
            "Epoch 8/10\n",
            "4582/4582 [==============================] - 69s 15ms/step - loss: 0.6925 - acc: 0.5146 - val_loss: 0.6839 - val_acc: 0.6889\n",
            "Epoch 9/10\n",
            "4582/4582 [==============================] - 70s 15ms/step - loss: 0.6925 - acc: 0.5378 - val_loss: 0.6919 - val_acc: 0.5046\n",
            "Epoch 10/10\n",
            "4582/4582 [==============================] - 69s 15ms/step - loss: 0.6924 - acc: 0.5347 - val_loss: 0.6911 - val_acc: 0.5217\n",
            "0.5470383272953749\n",
            "0.5216718264408525\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.56      0.22        79\n",
            "           1       0.89      0.52      0.65       567\n",
            "\n",
            "    accuracy                           0.52       646\n",
            "   macro avg       0.52      0.54      0.44       646\n",
            "weighted avg       0.80      0.52      0.60       646\n",
            "\n",
            "[[ 44  35]\n",
            " [274 293]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttok1RbJi59o",
        "colab_type": "code",
        "outputId": "b7963728-e468-48b6-8cce-fa431e44ce13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dfa = df5\n",
        "commenta_array = []\n",
        "train_target = []\n",
        "comtest_array = []\n",
        "test_words = []\n",
        "dfa['Comment'] = dfa['Comment'].astype(str)\n",
        "\n",
        "    # Remove punctuation marks and tokenize each and every word\n",
        "dfa['Comment'] = dfa['Comment'].str.replace('[^\\w\\s]', ' ')\n",
        "dfa['Comment'] = dfa['Comment'].str.replace('[\\d+]', ' ')\n",
        "dfa['Comment'] = dfa['Comment'].str.replace('(^| ).(( ).)*( |$)', ' ')\n",
        "\n",
        "dfa['Comment'] = dfa['Comment'].apply(lambda s : comment_cleaner(s, commenta_array))\n",
        "    # Split into positive and negative datasets\n",
        "pos8_df = dfa[dfa['Actual Polarity'] == 1]\n",
        "neg8_df = dfa[dfa['Actual Polarity'] == 0]\n",
        "neu8_df = dfa[dfa['Actual Polarity'] == 2]\n",
        "\n",
        "neu8_df['Length'] = neu8_df['Comment'].str.lower().str.split().apply(len)\n",
        "neu8_df = neu8_df[neu8_df['Length'] > 15]\n",
        "test8 = pd.concat([pos8_df,neg8_df])\n",
        "\n",
        "test8 = test8.reset_index(drop=True)\n",
        "print(test8['Comment'])\n",
        "    \n",
        "\n",
        "tka = Tokenizer(lower = True)\n",
        "tka.fit_on_texts(test8['Comment'])\n",
        "X_seqa = tk.texts_to_sequences(test8['Comment'])\n",
        "tfxa = pad_sequences(X_seqa,maxlen=268)\n",
        "print(tfxa.shape)\n",
        "\n",
        "test_target = test8['Actual Polarity']\n",
        "\n",
        "loss, accuracy = model.evaluate(tfxa, test_target, verbose=False)\n",
        "print(accuracy)\n",
        "    \n",
        "prima = model.predict_classes(tfxa)\n",
        "print(accuracy)\n",
        "print(classification_report(test_target,prima))\n",
        "print(confusion_matrix(test_target,prima))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0      haha ill post screenshot kaguya star chika boo...\n",
            "1      know show really popular chinese nicovideo rig...\n",
            "2      raise boy kinda hard believe shirogane unathle...\n",
            "3      wanna get good serve volleyball maybe open fuc...\n",
            "4      chika start rant society rich poor gap remind ...\n",
            "5                                       fuck chika dance\n",
            "6                                      banhahahahahahaha\n",
            "7                                   imagine post comment\n",
            "8      man talk every episode koga aoi fujiwaras va r...\n",
            "9      koga aoi like different kaguyas week tiny aaaa...\n",
            "10                          mamafujiwara best mom winter\n",
            "11     expect gonna voice one sexusu pistaruzu thats ...\n",
            "12                          ahem uncultured tenshi great\n",
            "13                                  make first major gig\n",
            "14     tenshi moderately popular light novel series a...\n",
            "15     go like three different voice give umbrella great\n",
            "16                                               perfect\n",
            "17              must practice day impress colleague cute\n",
            "18     absolutely love frustration try teach couldnt ...\n",
            "19     kaguyas va also amaze range ive actually cant ...\n",
            "20     yeah va brings game every episode amaze job re...\n",
            "21                  im ready cute compilation season end\n",
            "22                                      ah would perfect\n",
            "23                  much full episode worth kumiko noise\n",
            "24     probably theyre likely best enjoy separately i...\n",
            "25     theyre probably use record point koga aois ara...\n",
            "26     kaguya whine fujiwara ruin everything perfect ...\n",
            "27            also get horror quite right wish bit delay\n",
            "28     yeah fun love hear effort go set wow one deter...\n",
            "29               yeah amaze usual cute reaction adorable\n",
            "                             ...                        \n",
            "652                 im disappoint didnt include exchange\n",
            "653    anime overrate bad decent thing detective chik...\n",
            "654         fuck overrate minority dislike lol taste bad\n",
            "655           sad thing absolute melter thing every week\n",
            "656                thats even remotely similar art style\n",
            "657                                                     \n",
            "658                                                     \n",
            "659                                                     \n",
            "660                                                     \n",
            "661                                                     \n",
            "662                                            dude soon\n",
            "663                       discover uholofan life get ban\n",
            "664                  admins already send warn wait legit\n",
            "665                                                 cunt\n",
            "666    burn mod call arm brother sister time revoluti...\n",
            "667                                   weebs rise protest\n",
            "668                                 admins mod one fault\n",
            "669                             cant say hear everything\n",
            "670    let start call admins misogynist judge woman b...\n",
            "671    internet would look like got rid net neutralit...\n",
            "672          net neutrality already go coincidence think\n",
            "673               vice admiral scarred water disgraceful\n",
            "674                          oh damn get suspendedbanned\n",
            "675                                                 damn\n",
            "676                                         mean nothing\n",
            "677                                          life suffer\n",
            "678       manga spoilerswhy tsundere senpai exist suffer\n",
            "679    one one care another male romcom cant go home ...\n",
            "680                                          rip holofan\n",
            "681                                   people guild trash\n",
            "Name: Comment, Length: 682, dtype: object\n",
            "(682, 268)\n",
            "0.6041055718693565\n",
            "0.6041055718693565\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.32      0.17        84\n",
            "           1       0.87      0.64      0.74       598\n",
            "\n",
            "    accuracy                           0.60       682\n",
            "   macro avg       0.49      0.48      0.45       682\n",
            "weighted avg       0.78      0.60      0.67       682\n",
            "\n",
            "[[ 27  57]\n",
            " [213 385]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}